{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shiny import App, render, ui, reactive\n",
    "# from shinywidgets import output_widget, render_widget\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '/homes_unix/agillig/Projects/FabricOfCognition/code')\n",
    "import func_toolbox as ftools\n",
    "from func_toolbox import fetch_neurosynth_data\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "from nilearn import image\n",
    "\n",
    "project_dir = '/homes_unix/agillig/github_repos/ginna' #indicate path to the repository\n",
    "\n",
    "neurosynth_terms_file = project_dir + '/data/terms/BCS_3D.csv'\n",
    "os.makedirs(Path(neurosynth_terms_file).parent, exist_ok=True)\n",
    "\n",
    "# download the file from the Pacela et al. 2021 paper repo \n",
    "# https://github.com/vale-pak/BCS\n",
    "\n",
    "if not os.path.exists(neurosynth_terms_file):\n",
    "    fetch_neurosynth_data(f'{project_dir}/data')\n",
    "\n",
    "# https://github.com/vale-pak/BCS/blob/main/BCS_3D.csv\n",
    "df = pd.read_csv(neurosynth_terms_file, sep = ',')\n",
    "\n",
    "neurosynth_terms = df['Functions']\n",
    "\n",
    "fcu = ftools.Utilities()\n",
    "atlas_str = \"aicha-aal3\"\n",
    "\n",
    "# create a new directory for the analysis\n",
    "analysis_dir = project_dir + '/analysis/mean_RSNs_aicha-aal3'\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "\n",
    "mip_rsn_dir = project_dir + f'/Results/mip/mip_rsn_parcellated/aicha'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create atlas : aicha + AAL cerebellum / BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_atlas_file = f'{project_dir}/data/parcellation_atlases/aicha-aal3/parcels_aicha-aal3.nii.gz'\n",
    "\n",
    "# the atlas is already present in the github repository\n",
    "if not os.path.exists(parcellation_atlas_file):\n",
    "        atlases_dir = ''\n",
    "        file_cortical = atlases_dir + '/AICHA_v2_websiteC/AICHA.nii'\n",
    "        file_subcortical = atlases_dir + '/AAL3/AAL3v1.nii'\n",
    "\n",
    "        cortical_img = image.load_img(file_cortical)\n",
    "        new_data = np.zeros_like(cortical_img.get_fdata(), dtype = 'int32')\n",
    "\n",
    "        cortical_data = image.load_img(file_cortical).get_fdata()\n",
    "        subcortical_data = image.load_img(file_subcortical).get_fdata()\n",
    "        # cortical\n",
    "        max_cortical = np.max(cortical_data)\n",
    "        for i, value in enumerate(np.unique(cortical_data)[1:]):\n",
    "                new_data[cortical_data == value] = i + 1 \n",
    "        # cerebellum\n",
    "        # indices : from 95-120 for cerebellum; 121-170 for subcrotcial/brainstem\n",
    "        for i, value in enumerate([l for l in range(95 , 121)]):\n",
    "                new_data[subcortical_data == value] = max_cortical + i + 1 \n",
    "\n",
    "        new_img = image.new_img_like(cortical_img, new_data)\n",
    "        out_map = parcellation_atlas_file\n",
    "        os.makedirs(os.path.dirname(out_map), exist_ok=True)\n",
    "        # new_img.to_filename(out_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj = ftools.concatenate_all_rsn_projections()\n",
    "\n",
    "# fcu_mean = FC.utilities()\n",
    "# fcu_mean.analysis_dir = '/analysis/mean_RSNs'\n",
    "# proj_m = fcu_mean.concatenate_all_rsn_projections()\n",
    "\n",
    "# filestr = [f'rsn-{r:02d}'for r in df['rsn'].unique()]\n",
    "# rsn_int = [int(i) for i in proj['rsn'].unique()]\n",
    "\n",
    "# rsns to exclude\n",
    "# exclude = ['05', '34', '40'] # 34 bg, 40 artifact\n",
    "\n",
    "# rsn_int = [i for i in rsn_int if np.isin(f'{i:02d}', exclude) == False]\n",
    "\n",
    "# rsn_str = [f'{el:02d}' for el in rsn_int]\n",
    "# filestr = [f'{i:02d}' for i in rsn_int]\n",
    "\n",
    "# n_rsn = len(rsn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_dir = f'{project_dir}/atlas/zmaps'\n",
    "rsn_files = [os.path.join(atlas_dir, f) for f in os.listdir(atlas_dir) if f.endswith('.nii')]\n",
    "rsn_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create A 4D volume with all 506 maps of the model\n",
    "# this may take a few min\n",
    "terms_maps_dir = f'{project_dir}/data/dataset'\n",
    "terms_maps_files = [os.path.join(terms_maps_dir, f) for f in os.listdir(terms_maps_dir) if f.endswith('.nii.gz')]\n",
    "terms_maps_files.sort()\n",
    "\n",
    "out_dir = f'{terms_maps_dir}/concatenated'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "out_name = out_dir + '/dataset_concatenated.nii.gz'\n",
    "if os.path.isfile(out_name) == False:\n",
    "    image.concat_imgs(terms_maps_files).to_filename(out_name)\n",
    "concat_ds = image.load_img(out_name)\n",
    "\n",
    "terms_maps_files_all = terms_maps_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create & retrieve parcellated 506 meta analytic maps\n",
    "# should also take a few minutes, but needs to be done only once\n",
    "\n",
    "n_terms = concat_ds.shape[3]\n",
    "\n",
    "parcellated_dataset_dir = f'{project_dir}/data/dataset/parcellated/{atlas_str}'\n",
    "os.makedirs(parcellated_dataset_dir, exist_ok=True)\n",
    "\n",
    "parcellated_dataset_file = os.path.join(parcellated_dataset_dir, f'neurosynthterms_parcellations_{atlas_str}.csv')\n",
    "\n",
    "parcellated = []\n",
    "\n",
    "if os.path.isfile(parcellated_dataset_file) == False:\n",
    "    for t in range(n_terms):\n",
    "        temp_img = image.index_img(concat_ds, t)\n",
    "        parcellated.append(fcu.parcellate(temp_img, atlas=parcellation_atlas_file))\n",
    "\n",
    "    parcellated = np.array(parcellated).squeeze()\n",
    "    terms = neurosynth_terms.values\n",
    "    # print(parcellated.shape)\n",
    "    parcels = [i for i in range(1, parcellated.shape[1] +1)]\n",
    "    df = pd.DataFrame(parcellated, index=terms, columns = parcels)\n",
    "    df.to_csv(parcellated_dataset_file)\n",
    "\n",
    "dataset_parcellated = pd.read_csv(parcellated_dataset_file, sep = ',', index_col = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each rsn, compute the spatial correlation with the 506 meta analytic maps (parcellated), sort\n",
    "atlas_img = '/homes_unix/agillig/Atlases/RSN_N41_zNpair_clean1.nii'\n",
    "spatial_correlation = {}\n",
    "parcellation_dir = analysis_dir + '/parcellations'\n",
    "os.makedirs(parcellation_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "rsn_str = pd.DataFrame(rsn_files).iloc[:,0].str.split('/').str[-1].str[-6:-4]\n",
    "\n",
    "for i, rsn in enumerate(rsn_str):\n",
    "    parcellation_file = parcellation_dir + f'/rsn-{rsn}/rsn-{rsn}_unique_parcellated.csv'\n",
    "    os.makedirs(os.path.dirname(parcellation_file), exist_ok=True)\n",
    "\n",
    "    index = int(rsn) - 1\n",
    "    # tmp_data = image.index_img(atlas_img, index)\n",
    "    # affine = tmp_img.affine\n",
    "    if os.path.isfile(parcellation_file) == False:\n",
    "        tmp_img = image.load_img(rsn_files[index])\n",
    "        tmp_parcellatd = fcu.parcellate(tmp_img, atlas=parcellation_atlas_file)\n",
    "        np.savetxt(parcellation_file, tmp_parcellatd, delimiter = ',')\n",
    "    data = np.loadtxt(parcellation_file, delimiter = ',')\n",
    "\n",
    "\n",
    "    corr_temp = [np.corrcoef(data, dataset_parcellated.iloc[j,:])[1,0] for j in range(n_terms)]\n",
    "\n",
    "    # corr_temp = []\n",
    "    # for j, (term, file) in enumerate(zip(BCSterms, BCS_maps_files)): \n",
    "    #     img_comparison = image.index_img(concat_ds, j)\n",
    "    #     data_comparison = img_comparison.get_fdata().copy()\n",
    "    #     corr_temp.append(np.corrcoef(data.flatten(), data_comparison.flatten())[1,0])\n",
    "    spatial_correlation[rsn] = corr_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non parametric statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_projections = {}\n",
    "# use brainsmash to generate surrogate data that preserves spatial autocorrelation (Burt, 2020)\n",
    "# https://github.com/murraylab/brainsmash: cf null_parcellations.py\n",
    "\n",
    "# once computed, for each rsn, compute the correlation of the null parcellations with the 506 meta analytic maps (parcellated)\n",
    "\n",
    "n_perm = 100 # number of permutations per batch\n",
    "n_batches = 1 #Â number of batches. can be useful to split the work in a cluster environment\n",
    "total_n_perm = int(n_perm * n_batches)\n",
    "\n",
    "null_dir = analysis_dir + '/null_correlations'\n",
    "def compute_correlation_null(rsn, n_perm = 1000, n_batches = 10, overwrite = False):\n",
    "    print(f'processing rsn {rsn}')\n",
    "\n",
    "    null_dir = analysis_dir + '/null_correlations'\n",
    "    save_dir = null_dir + f'/rsn-{rsn}'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_file = os.path.join(save_dir, f'rsn-{rsn}_null_correlations.csv')    \n",
    "\n",
    "    if os.path.isfile(save_file) and overwrite == False:\n",
    "        print('file already exists; no overwriting')\n",
    "        return\n",
    "    \n",
    "    data = []\n",
    "    for batch in range(1,n_batches+1):\n",
    "        null_prcl_dir = analysis_dir + '/null_parcellations'\n",
    "        null_parcellations_file = null_prcl_dir + f'/rsn-{rsn}/rsn-{rsn}_null_parcellations_batch-{batch:02d}_of_{n_batches}.csv'\n",
    "        temp_data = np.loadtxt(null_parcellations_file, delimiter = ' ')\n",
    "        data.append(temp_data)\n",
    "    data = np.concatenate(data, axis = 0)\n",
    "    # print(f'shape of data: {data.shape}')\n",
    "\n",
    "    index = int(rsn) - 1\n",
    "\n",
    "    # Get the target series\n",
    "    corr_temp = []\n",
    "    for j in range(n_terms):\n",
    "        \n",
    "        term_single = dataset_parcellated.iloc[j,:].values\n",
    "        # Compute the Pearson correlation coefficient for each column\n",
    "        corr_temp.append([np.corrcoef(data.T[:, i], term_single)[0,1] for i in range(n_perm * n_batches)])\n",
    "\n",
    "        # corr_temp = []\n",
    "        # for j, (term, file) in enumerate(zip(BCSterms, BCS_maps_files)): \n",
    "        #     img_comparison = image.index_img(concat_ds, j)\n",
    "        #     data_comparison = img_comparison.get_fdata().copy()\n",
    "        #     corr_temp.append(np.corrcoef(data.flatten(), data_comparison.flatten())[1,0])\n",
    "    corr_temp = np.array(corr_temp)\n",
    "    # spatial_correlation_null[rsn] = corr_temp\n",
    "    # save to file\n",
    "    print('saving file')\n",
    "    np.savetxt(save_file, corr_temp) \n",
    "    return corr_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save null correlations for each rsn\n",
    "# from multiprocessing import Pool\n",
    "# spatial_correlation_null = {}\n",
    "# with Pool() as pool:\n",
    "#     pool.map(compute_correlation_null, rsn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing rsn 01\n",
      "batch 1\n",
      "generating 10 surrogates\n",
      "elapsed time: 2.85 s\n",
      "saving surrogate maps to /homes_unix/agillig/github_repos/ginna/analysis/mean_RSNs_aicha-aal3/null_parcellations/rsn-01/rsn-01_null_parcellations_batch-01_of_1.csv\n"
     ]
    }
   ],
   "source": [
    "from null_parcellations import generate_null\n",
    "import importlib\n",
    "import null_parcellations\n",
    "importlib.reload(null_parcellations)\n",
    "null_parcellations.generate_null(1, project_dir, n_perm=10, n_batches=n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing rsn 01\n",
      "saving file\n"
     ]
    }
   ],
   "source": [
    "res = compute_correlation_null('01', n_perm=10, n_batches=1, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading null correlations...\n"
     ]
    }
   ],
   "source": [
    "# load null correlations\n",
    "\n",
    "null_correlations = []\n",
    "null_correlations_dir = analysis_dir + '/null_correlations'\n",
    "null_prcl_dir = analysis_dir + 'null_parcellations'\n",
    "\n",
    "# from brainsmash.mapgen.base import Base\n",
    "n_perm = 10000\n",
    "n_batches = 10\n",
    "\n",
    "\n",
    "# os.makedirs(null_dir, exist_ok=True)\n",
    "print(f'loading null correlations...')\n",
    "for ind, rsn in enumerate(rsn_str):\n",
    "    \n",
    "    save_dir = null_correlations_dir + f'/rsn-{rsn}'\n",
    "\n",
    "    save_file = os.path.join(save_dir, os.listdir(save_dir)[0])\n",
    "        \n",
    "    null_correlations.append(np.loadtxt(save_file))\n",
    "                             \n",
    "null_correlations = np.asarray(null_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. compute p-value for each term / RSN association\n",
    "# compute p-value for each term / RSN association\n",
    "n_terms = len(neurosynth_terms)\n",
    "# p_val_list = []\n",
    "\n",
    "def compute_all_pval_uncor(ind):\n",
    "    rsn = rsn_str[ind]\n",
    "\n",
    "    null_distribution_dir = null_dir + f'/rsn-{rsn}'\n",
    "    null_distribution_file =  os.path.join(null_distribution_dir, f'rsn-{rsn}_null_correlations.csv')  \n",
    "\n",
    "    # print(f'processing rsn {rsn}')\n",
    "    parcellation_file = parcellation_dir + f'/rsn-{rsn}/rsn-{rsn}_unique_parcellated.csv'\n",
    "    data_parcellated = np.loadtxt(parcellation_file, delimiter = ',')\n",
    "    temp_pvals = []\n",
    "    observed_corr_list = []\n",
    "    null_distribution = np.loadtxt(null_distribution_file, delimiter = ' ')\n",
    "    tmp_ind_obs = []\n",
    "    for t in range(n_terms): \n",
    "        # print(f'BCS term: {BCSterms[t]}')\n",
    "        \n",
    "        observed_corr = spatial_correlation[rsn][t]\n",
    "        # print(f'observed correlation: {observed_corr}')\n",
    "        observed_corr_list.append(observed_corr)\n",
    "        null_corr= []\n",
    "\n",
    "        # without correction for multiple comparisons\n",
    "        null_distribution_term = null_distribution[t,:]\n",
    "        distr_sorted = np.sort(null_distribution_term)\n",
    "\n",
    "        n_val = len(distr_sorted) + 1\n",
    "        ind_obs = n_val - np.searchsorted(distr_sorted, observed_corr) #+1 # +1 because python indices start at 0\n",
    "        # print(ind_obs)\n",
    "        tmp_ind_obs.append(ind_obs)\n",
    "        p_val = ind_obs / n_val\n",
    "        temp_pvals.append(p_val)\n",
    "    temp_pvals = np.asarray(temp_pvals)\n",
    "    tmp_ind_obs = np.asarray(tmp_ind_obs)\n",
    "    observed_corr_list = np.asarray(observed_corr_list)\n",
    "    # p_val_list.append(temp_pvals)\n",
    "    output = {'pvals': temp_pvals, 'indice': tmp_ind_obs, 'observed_correlation': observed_corr_list}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "with Pool() as pool:\n",
    "    results_uncor = pool.map(compute_all_pval_uncor, range(len(rsn_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_all_uncor = np.asarray([results_uncor[i]['pvals'] for i in range(len(rsn_str))])\n",
    "observed_correlations_all_uncor = np.asarray([results_uncor[i]['observed_correlation'] for i in range(len(rsn_str))])\n",
    "\n",
    "p_arr = np.hstack(pvals_all_uncor)\n",
    "correlation_arr = np.hstack(observed_correlations_all_uncor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction for multiple comparisons\n",
    "repeat the same but with correction for multiple comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximal statistics correction accross the terms (not rsn)\n",
    "def compute_null_distr_corr(rsn):\n",
    "    _n_perm = 10000\n",
    "\n",
    "    # load null distribution (n_parcels x n_perm)\n",
    "    null_distribution_dir = null_dir + f'/rsn-{rsn}'\n",
    "    null_distribution_file =  os.path.join(null_distribution_dir, f'rsn-{rsn}_null_correlations.csv')  \n",
    "    # print(f'processing rsn {rsn}')\n",
    "    null_distribution = np.loadtxt(null_distribution_file, delimiter = ' ')\n",
    "\n",
    "    corrected_null_distribution = np.max(null_distribution, axis=0)\n",
    "    \n",
    "\n",
    "    save_dir = analysis_dir + f'/null_correlations_maxcorc/rsn-{rsn}'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    savefile = save_dir + f'/rsn-{rsn}_null_correlations_maxcorc.csv'\n",
    "\n",
    "    np.savetxt(savefile, corrected_null_distribution, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "with Pool() as pool:\n",
    "    pool.map(compute_null_distr_corr, rsn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction function accross terms\n",
    "def compute_all_pval_cor(ind):\n",
    "    rsn = rsn_str[ind]\n",
    "\n",
    "    null_distribution_dir = analysis_dir + f'/null_correlations_maxcorc/rsn-{rsn}'\n",
    "    null_distribution_file =  os.path.join(null_distribution_dir, f'rsn-{rsn}_null_correlations_maxcorc.csv')  \n",
    "\n",
    "    # print(f'processing rsn {rsn}')\n",
    "    temp_pvals = []\n",
    "    observed_corr_list_corc = []\n",
    "    null_distribution = np.loadtxt(null_distribution_file, delimiter = ' ')\n",
    "    distr_sorted = np.sort(null_distribution)\n",
    "\n",
    "    tmp_ind_obs = []\n",
    "    for t in range(n_terms): \n",
    "        # print(f'BCS term: {BCSterms[t]}')\n",
    "        \n",
    "        observed_corr = spatial_correlation[rsn][t]\n",
    "        # print(f'observed correlation: {observed_corr}')\n",
    "        observed_corr_list_corc.append(observed_corr)\n",
    "        null_corr= []\n",
    "\n",
    "        # without correction for multiple comparisons\n",
    "        # null_distribution_term = null_distribution\n",
    "        # distr_sorted = np.sort(null_distribution_term)\n",
    "\n",
    "        n_val = len(distr_sorted) + 1\n",
    "        ind_obs = n_val - np.searchsorted(distr_sorted, observed_corr) #+1 # +1 because python indices start at 0\n",
    "        # print(ind_obs)\n",
    "        tmp_ind_obs.append(ind_obs)\n",
    "        p_val = ind_obs / n_val\n",
    "        temp_pvals.append(p_val)\n",
    "    temp_pvals = np.asarray(temp_pvals)\n",
    "    tmp_ind_obs = np.asarray(tmp_ind_obs)\n",
    "    observed_corr_list_corc = np.asarray(observed_corr_list_corc)\n",
    "    # p_val_list.append(temp_pvals)\n",
    "    output = {'pvals': temp_pvals, 'indice': tmp_ind_obs, 'observed_correlation': observed_corr_list_corc}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "with Pool() as pool:\n",
    "    results_cor = pool.map(compute_all_pval_cor, range(len(rsn_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "savefile = project_dir + '/Results/files/aicha/rsn_p_maxcorc_termwise.csv'\n",
    "os.makedirs(os.path.dirname(savefile), exist_ok=True)\n",
    "save_correlation = {}\n",
    "save_cor = {}\n",
    "save_p_uncor = {}\n",
    "for i, rsn in enumerate(rsn_str):\n",
    "    save_cor[rsn] = results_cor[i]['pvals'].tolist()\n",
    "    save_p_uncor[rsn] = results_uncor[i]['pvals'].tolist()\n",
    "    save_correlation[rsn] = results_cor[i]['observed_correlation'].tolist()\n",
    "# save p values\n",
    "    # corrected\n",
    "with open(savefile, 'w') as f:\n",
    "    json.dump(save_cor, f)\n",
    "    # uncorrected\n",
    "savefile = project_dir + '/Results/files/aicha/rsn_p_uncor.csv'\n",
    "with open(savefile, 'w') as f:\n",
    "    json.dump(save_p_uncor, f)\n",
    "\n",
    "# save correlations\n",
    "savefile = project_dir + '/Results/files/aicha/rsn_Pearsonr.csv'\n",
    "with open(savefile, 'w') as f:\n",
    "    json.dump(save_correlation, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_all_cor = np.asarray([results_cor[i]['pvals'] for i in range(len(rsn_str))])\n",
    "observed_correlations_all_cor = np.asarray([results_cor[i]['observed_correlation'] for i in range(len(rsn_str))])\n",
    "\n",
    "pcor_arr = np.hstack(pvals_all_cor)\n",
    "correlationcor_arr = np.hstack(observed_correlations_all_cor)\n",
    "\n",
    "# Assume groups is a list or array of the same length as correlation_arr and p_arr\n",
    "# that defines the group for each point\n",
    "groups = np.hstack([[rsn for i in range(pvals_all_cor[0].shape[0])] for rsn in rsn_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_terms = {}\n",
    "pvals = {}\n",
    "selected_terms_list = []\n",
    "pvals_list = []\n",
    "for i, rsn in enumerate(rsn_str):\n",
    "    selected_terms[rsn] = neurosynth_terms[results_cor[i]['pvals'] < 0.05].tolist()\n",
    "    pvals[rsn] = results_cor[i]['pvals'][results_cor[i]['pvals'] < 0.05]\n",
    "    selected_terms_list.append(neurosynth_terms[results_cor[i]['pvals'] < 0.05].tolist())\n",
    "    pvals_list.append(results_cor[i]['pvals'][results_cor[i]['pvals'] < 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "rsn_row_id = [np.repeat(rstr, 1).tolist() for i, rstr in enumerate(rsn_str)]\n",
    "rsn_row_id = np.asarray(list(chain.from_iterable(rsn_row_id)))\n",
    "cluster_row_id = [f'{1:02d}' for i in range(n_rsn)] \n",
    "cluster_row_id = np.asarray(list(chain.from_iterable(cluster_row_id)))\n",
    "indexing_array = [rsn_row_id, cluster_row_id]\n",
    "rsn_id_group = rsn_row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_terms_sel = []\n",
    "str_terms_sel_p = []\n",
    "str_p_sel = []\n",
    "list_dist_sel = []\n",
    "\n",
    "for i, rsn in enumerate(rsn_str):\n",
    "\n",
    "        dst = observed_correlations_all_cor[i]\n",
    "        pval = pvals_all_cor[i] \n",
    "\n",
    "        sel_terms_p = neurosynth_terms[pval < 0.05].tolist()\n",
    "        sel_p = pval[pval < 0.05]\n",
    "\n",
    "        def format_decimal(val):\n",
    "                # s = str(val)\n",
    "                s = \"{:.15f}\".format(val)  # Convert the number to a string with 15 decimal places; avoids scientific notation bypass of method\n",
    "                if '.' in s:\n",
    "                        pre, post = s.split('.')\n",
    "                        first_non_zero = next((i for i, c in enumerate(post) if c != '0'), len(post))\n",
    "                        return f'{{:.{first_non_zero + 1}f}}'.format(val)\n",
    "                else:\n",
    "                        return s\n",
    "\n",
    "        sel_p = [format_decimal(float(i)) for i in sel_p]\n",
    "\n",
    "        # sel_p = [f'{float(i):.1f}'.lstrip('0') for i in sel_p]\n",
    "        sel_terms_p = [f'{tstr} ({pstr})' for tstr, pstr in zip(sel_terms_p, sel_p)]\n",
    "\n",
    "        # list_terms_sel.append(BCSterms[np.argsort(d)][:k])\n",
    "        tstr = ', '.join(sel_terms_p)\n",
    "        pstr = ', '.join(sel_p)\n",
    "        # list_dist_sel.append(d[np.argsort(d)][:k])\n",
    "        str_terms_sel_p.append(np.asarray(tstr, dtype='str'))\n",
    "        str_p_sel.append(np.asarray(pstr, dtype='str'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
