{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS ###\n",
    "project_dir = '/homes_unix/agillig/github_repos/ginna' #specify your path to the github repository\n",
    "\n",
    "n_perm = 100 # number of permutations per batch; reduced for the sake of the example (publication: 10,000)\n",
    "n_batches = 1 # number of batches. can be useful to split the work in a cluster environment\n",
    "\n",
    "compute_all_rsns = False # if True, compute all RSNs; if False, compute only RSN05 as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, project_dir + '/code')\n",
    "\n",
    "import func_toolbox as ftools\n",
    "from func_toolbox import fetch_neurosynth_data\n",
    "from nilearn import image\n",
    "import null_parcellations\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "neurosynth_terms_file = project_dir + '/data/terms/BCS_3D.csv'\n",
    "os.makedirs(Path(neurosynth_terms_file).parent, exist_ok=True)\n",
    "\n",
    "# download the file from the Pacela et al. 2021 paper repo \n",
    "# https://github.com/vale-pak/BCS\n",
    "\n",
    "if not os.path.exists(neurosynth_terms_file):\n",
    "    fetch_neurosynth_data(f'{project_dir}/data')\n",
    "\n",
    "# https://github.com/vale-pak/BCS/blob/main/BCS_3D.csv\n",
    "df = pd.read_csv(neurosynth_terms_file, sep = ',')\n",
    "\n",
    "neurosynth_terms = df['Functions']\n",
    "\n",
    "fcu = ftools.Utilities()\n",
    "atlas_str = \"aicha-aal3\"\n",
    "\n",
    "fcu.set_project_dir(project_dir)\n",
    "fcu.set_atlas_name(atlas_str)\n",
    "\n",
    "# create a new directory for the analysis\n",
    "analysis_dir = project_dir + f'/analysis/mean_RSNs_{atlas_str}'\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "\n",
    "mip_rsn_dir = project_dir + f'/Results/mip/mip_rsn_parcellated/aicha'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create atlas : aicha + AAL cerebellum / BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcellation_atlas_file = f'{project_dir}/data/parcellation_atlases/aicha-aal3/parcels_aicha-aal3.nii.gz'\n",
    "\n",
    "# the atlas is already present in the github repository\n",
    "if not os.path.exists(parcellation_atlas_file):\n",
    "        atlases_dir = ''\n",
    "        file_cortical = atlases_dir + '/AICHA_v2_websiteC/AICHA.nii'\n",
    "        file_subcortical = atlases_dir + '/AAL3/AAL3v1.nii'\n",
    "\n",
    "        cortical_img = image.load_img(file_cortical)\n",
    "        new_data = np.zeros_like(cortical_img.get_fdata(), dtype = 'int32')\n",
    "\n",
    "        cortical_data = image.load_img(file_cortical).get_fdata()\n",
    "        subcortical_data = image.load_img(file_subcortical).get_fdata()\n",
    "        # cortical\n",
    "        max_cortical = np.max(cortical_data)\n",
    "        for i, value in enumerate(np.unique(cortical_data)[1:]):\n",
    "                new_data[cortical_data == value] = i + 1 \n",
    "        # cerebellum\n",
    "        # indices : from 95-120 for cerebellum; 121-170 for subcrotcial/brainstem\n",
    "        for i, value in enumerate([l for l in range(95 , 121)]):\n",
    "                new_data[subcortical_data == value] = max_cortical + i + 1 \n",
    "\n",
    "        new_img = image.new_img_like(cortical_img, new_data)\n",
    "        out_map = parcellation_atlas_file\n",
    "        os.makedirs(os.path.dirname(out_map), exist_ok=True)\n",
    "        new_img.to_filename(out_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to GINNA zstat maps\n",
    "atlas_dir = f'{project_dir}/atlas/zmaps'\n",
    "rsn_files = [os.path.join(atlas_dir, f) for f in os.listdir(atlas_dir) if f.endswith('.nii')]\n",
    "rsn_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create A 4D volume with all 506 maps of the model\n",
    "# this may take a few min\n",
    "terms_maps_dir = f'{project_dir}/data/dataset'\n",
    "terms_maps_files = [os.path.join(terms_maps_dir, f) for f in os.listdir(terms_maps_dir) if f.endswith('.nii.gz')]\n",
    "terms_maps_files.sort()\n",
    "\n",
    "out_dir = f'{terms_maps_dir}/concatenated'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "out_name = out_dir + '/dataset_concatenated.nii.gz'\n",
    "if os.path.isfile(out_name) == False:\n",
    "    image.concat_imgs(terms_maps_files).to_filename(out_name)\n",
    "concat_ds = image.load_img(out_name)\n",
    "\n",
    "terms_maps_files_all = terms_maps_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create & retrieve parcellated 506 meta analytic maps\n",
    "# should also take a few minutes, but needs to be done only once\n",
    "\n",
    "n_terms = concat_ds.shape[3]\n",
    "\n",
    "parcellated_dataset_dir = f'{project_dir}/data/dataset/parcellated/{atlas_str}'\n",
    "os.makedirs(parcellated_dataset_dir, exist_ok=True)\n",
    "\n",
    "parcellated_dataset_file = os.path.join(parcellated_dataset_dir, f'neurosynthterms_parcellations_{atlas_str}.csv')\n",
    "\n",
    "parcellated = []\n",
    "\n",
    "if os.path.isfile(parcellated_dataset_file) == False:\n",
    "    for t in range(n_terms):\n",
    "        temp_img = image.index_img(concat_ds, t)\n",
    "        parcellated.append(fcu.parcellate(temp_img, atlas=parcellation_atlas_file))\n",
    "\n",
    "    parcellated = np.array(parcellated).squeeze()\n",
    "    terms = neurosynth_terms.values\n",
    "    # print(parcellated.shape)\n",
    "    parcels = [i for i in range(1, parcellated.shape[1] +1)]\n",
    "    dataset_parcellated = pd.DataFrame(parcellated, index=terms, columns = parcels)\n",
    "    dataset_parcellated.to_csv(parcellated_dataset_file)\n",
    "else:\n",
    "    dataset_parcellated = pd.read_csv(parcellated_dataset_file, sep = ',', index_col = 0)\n",
    "\n",
    "fcu.dataset_parcellated = dataset_parcellated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial correlation between RSNs & the Neurosynth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each rsn, compute the spatial correlation with the 506 meta analytic maps (parcellated)\n",
    "atlas_img = '/homes_unix/agillig/Atlases/RSN_N41_zNpair_clean1.nii'\n",
    "parcellation_dir = analysis_dir + '/parcellations'\n",
    "os.makedirs(parcellation_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "rsn_str = pd.DataFrame(rsn_files).iloc[:,0].str.split('/').str[-1].str[-6:-4]\n",
    "\n",
    "if compute_all_rsns == False:\n",
    "    rsn_str = [rsn_str[4]] #limited to RSN05 to reduce computation time\n",
    "\n",
    "for i, rsn in enumerate(rsn_str):\n",
    "    parcellation_file = parcellation_dir + f'/rsn-{rsn}/rsn-{rsn}_unique_parcellated.csv'\n",
    "    os.makedirs(os.path.dirname(parcellation_file), exist_ok=True)\n",
    "\n",
    "    index = int(rsn) - 1\n",
    "    # tmp_data = image.index_img(atlas_img, index)\n",
    "    # affine = tmp_img.affine\n",
    "    if os.path.isfile(parcellation_file) == False:\n",
    "        tmp_img = image.load_img(rsn_files[index])\n",
    "        tmp_parcellatd = fcu.parcellate(tmp_img, atlas=parcellation_atlas_file)\n",
    "        np.savetxt(parcellation_file, tmp_parcellatd, delimiter = ',')\n",
    "    data = np.loadtxt(parcellation_file, delimiter = ',')\n",
    "\n",
    "\n",
    "    corr_temp = [np.corrcoef(data, dataset_parcellated.iloc[j,:])[1,0] for j in range(n_terms)]\n",
    "    \n",
    "    results[rsn] = {'term': neurosynth_terms, 'spatial_correlation': corr_temp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non parametric statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing rsn 05\n",
      "batch 1\n",
      "generating 100 surrogates\n",
      "elapsed time: 24.85 s\n",
      "saving surrogate maps to /homes_unix/agillig/github_repos/ginna/analysis/mean_RSNs_aicha-aal3/null_parcellations/rsn-05/rsn-05_null_parcellations_batch-01_of_1.csv\n",
      "processing rsn 05\n",
      "saving file\n"
     ]
    }
   ],
   "source": [
    "# null_projections = {}\n",
    "# use brainsmash to generate surrogate data that preserves spatial autocorrelation (Burt, 2020)\n",
    "# https://github.com/murraylab/brainsmash: cf null_parcellations.py\n",
    "\n",
    "# once computed, for each rsn, compute the correlation of the null parcellations with the 506 meta analytic maps (parcellated)\n",
    "\n",
    "for rsn in rsn_str:\n",
    "    null_parcellations.generate_null(int(rsn), project_dir, n_perm=n_perm, n_batches=n_batches)\n",
    "    null_distr = fcu.compute_correlation_null(rsn=rsn, n_perm=n_perm, n_batches=n_batches, overwrite=True) #set overwrite to False if you have already computed the null distributions\n",
    "\n",
    "    p, pcor = fcu.compute_pvalues(results[rsn]['spatial_correlation'], null_distr)\n",
    "\n",
    "    results[rsn]['p'] = p\n",
    "    results[rsn]['pcor'] = pcor\n",
    "    results[rsn]['is_significant'] = (results[rsn]['pcor'] < 0.05).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results summary for RSN05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>spatial_correlation</th>\n",
       "      <th>p</th>\n",
       "      <th>pcor</th>\n",
       "      <th>is_significant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>money</td>\n",
       "      <td>0.556019</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>preferences</td>\n",
       "      <td>0.497313</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>decision_making</td>\n",
       "      <td>0.471232</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>choice</td>\n",
       "      <td>0.454076</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>value</td>\n",
       "      <td>0.451444</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>visual</td>\n",
       "      <td>-0.116171</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>motor_imagery</td>\n",
       "      <td>-0.118895</td>\n",
       "      <td>0.772277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>imagery</td>\n",
       "      <td>-0.122966</td>\n",
       "      <td>0.811881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>movements</td>\n",
       "      <td>-0.125567</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>motor</td>\n",
       "      <td>-0.126094</td>\n",
       "      <td>0.782178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                term  spatial_correlation         p      pcor  is_significant\n",
       "258            money             0.556019  0.009901  0.009901            True\n",
       "321      preferences             0.497313  0.009901  0.009901            True\n",
       "84   decision_making             0.471232  0.009901  0.009901            True\n",
       "49            choice             0.454076  0.009901  0.019802            True\n",
       "468            value             0.451444  0.009901  0.019802            True\n",
       "..               ...                  ...       ...       ...             ...\n",
       "482           visual            -0.116171  0.732673  1.000000           False\n",
       "267    motor_imagery            -0.118895  0.772277  1.000000           False\n",
       "194          imagery            -0.122966  0.811881  1.000000           False\n",
       "273        movements            -0.125567  0.732673  1.000000           False\n",
       "265            motor            -0.126094  0.782178  1.000000           False\n",
       "\n",
       "[506 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_table = pd.DataFrame.from_dict(results['05']).sort_values(by='spatial_correlation', ascending=False)\n",
    "res_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complementary: Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rsn 05\n",
      "explained variance: [0.28079161 0.25684348 0.19920988 0.14174494 0.1214101 ]\n",
      "ordered loadings for PC01:\n",
      "                     PC01      PC02      PC03      PC04      PC05\n",
      "preferences      0.557958 -0.151097 -0.493970 -0.582350  0.287603\n",
      "decision_making  0.508642  0.322498  0.525055 -0.299288 -0.521558\n",
      "value            0.507006 -0.439341 -0.221607  0.613673 -0.352450\n",
      "choice           0.380679  0.616471  0.029275  0.441104  0.528789\n",
      "money            0.167317 -0.547822  0.656007 -0.011341  0.491349\n"
     ]
    }
   ],
   "source": [
    "# z-score threshold for the RSN mask\n",
    "thr = 1\n",
    "\n",
    "for i, rsn in enumerate(rsn_str):\n",
    "\n",
    "    rsn_results = results[rsn]\n",
    "\n",
    "    n_components = 10\n",
    "    print(f'rsn {rsn}')\n",
    "    # terms_maps_files[pvals_all_uncor[rsn_str.index(rsn)] < 0.05]\n",
    "    # uncorrected\n",
    "    \n",
    "    n_pca_components = np.min((n_components,len(np.nonzero(rsn_results['is_significant'])[0])))\n",
    "\n",
    "    pca = PCA(n_components=n_pca_components, \n",
    "              svd_solver='full')\n",
    "\n",
    "    # add masking of data with rsn map before computing pca\n",
    "    rsn_img = image.load_img(rsn_files[i])\n",
    "\n",
    "    rsn_data = fcu.parcellate(rsn_img, atlas=parcellation_atlas_file).squeeze()\n",
    "\n",
    "\n",
    "    # create a binary mask of the RSN map\n",
    "    rsn_mask = np.array([1 if i > thr else 0 for i in rsn_data])\n",
    "    rsn_regions = np.where(rsn_mask == 1)[0]\n",
    "\n",
    "\n",
    "    rsn_terms_indices = np.where(rsn_results['is_significant'])[0]\n",
    "    rsn_terms = neurosynth_terms.iloc[rsn_terms_indices]\n",
    "    # print(f'significant terms for rsn {rsn}: {rsn_terms.values}')\n",
    "    # skip if less than 3 terms are significant\n",
    "    if len(rsn_terms_indices) < 3:\n",
    "        print('less than 3 terms are significant, skipping')\n",
    "        continue\n",
    "\n",
    "\n",
    "    input_rsn_data = dataset_parcellated.iloc[rsn_terms_indices, rsn_regions].T\n",
    "\n",
    "    input_rsn_data = zscore(input_rsn_data)\n",
    "    x_new = pca.fit_transform(input_rsn_data)\n",
    "\n",
    "    print(f'explained variance: {pca.explained_variance_ratio_}')\n",
    "    rsn_results['pca'] = pca\n",
    "\n",
    "    pca_results = pd.DataFrame(pca.components_, \n",
    "                               index=[f'PC{c:02d}' for c in range(1, pca.components_.shape[0] + 1)], \n",
    "                               columns = rsn_terms.values).T\n",
    "\n",
    "    # loadings can be sorted in the following manner\n",
    "    print(f'ordered loadings for PC01:')\n",
    "    print(pca_results.sort_values(by='PC01', ascending=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
